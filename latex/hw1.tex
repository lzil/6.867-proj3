\documentclass[10pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\DeclareMathOperator{\sign}{sgn}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{float}

\usepackage[font=small,labelfont=bf]{caption}


\title{\vspace{-4ex}Neural Networks\vspace{-3.5ex}}
\begin{document}
\newgeometry{top=0.75in,left=0.75in,right=0.75in,bottom=1in}
\maketitle
\vspace{-0.5em}
\begin{abstract}
In this short paper we will discuss the fundamentals of neural networks and their implementation in detail. We will give a general overview of how neural networks work, discuss calculation of the gradient and implementation of back-propagation, and test our results on some real MNIST code.
\end{abstract}
Neural networks have been around for at least a few decades, but only recently have them become popular as a method for learning parameters that can correctly translate an input into an output. This is because of increased computational power, a greater availability of training data, as well as the fact that more complex models, like deep neural nets, are actually easy to train - the same back-propagation that works to update normal neural networks works just as well for multiple hidden layers.
\end{document}













